# ðŸ“š All About Agents

Welcome to our comprehensive resource collection for AI agents. This page curates valuable tools, frameworks, research papers, and learning materials to help you understand and build sophisticated agent systems.

## Table of Contents

1. [Agent Frameworks](#agent-frameworks)
2. [Agent Memory](#agent-memory)
3. [Papers](#papers)
4. [Evaluation](#evaluation)

## Agent Frameworks

- MiroFlow: Build, manage, and scale your AI agents with ease
    - [[GitHub]](https://github.com/MiroMindAI/MiroFlow)

- Youtu-Agent: A simple yet powerful agent framework that delivers with open-source models
    - [[GitHub]](https://github.com/TencentCloudADP/youtu-agent)

- OpenManus: No fortress, purely open ground. OpenManus is Coming
    - [[GitHub]](https://github.com/FoundationAgents/OpenManus)

- OpenBB Platform: Financial data platform for analysts, quants and AI agents 
    - [[Project]](https://github.com/OpenBB-finance/OpenBB)

### Agent Memory

- Mem0: Building Production- Ready AI Agents with Scalable Long-Term Memory
    - [[GitHub]](https://github.com/mem0ai/mem0)

- memobase: Profile-Based Long-Term Memory for AI Applications
    - [[GitHub]](https://github.com/memodb-io/memobase)

- Memento: Fine-tuning LLM Agents without Fine-tuning LLMs
    - [[Paper]](https://arxiv.org/abs/2508.16153), [[GitHub]](https://github.com/Agent-on-the-Fly/Memento)


### Papers

- Profile-Aware Maneuvering: A Dynamic Multi-Agent System for Robust GAIA Problem Solving by AWorld 
    - [[Paper]](https://arxiv.org/abs/2508.09889)

- AFlow: Automating Agentic Workflow Generation 
    - [[Paper]](https://arxiv.org/abs/2410.10762)

- AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs 
    - [[Paper]](https://arxiv.org/abs/2508.16153v2)

- Throttling Web Agents Using Reasoning Gates
    - [[Paper]](https://arxiv.org/abs/2509.01619)

- The Landscape of Agentic Reinforcement Learning for LLMs: A Survey
    - [[Paper]](https://arxiv.org/abs/2509.02547)



### Evaluation

- LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries 
    - [[Paper]](https://arxiv.org/abs/2508.15760)

- BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent 
    - [[Paper]](https://arxiv.org/abs/2508.06600)

- HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering
    -  [[Paper]](https://arxiv.org/abs/1809.09600)

- GAIA: a benchmark for General AI Assistants 
    - [[Paper]](https://arxiv.org/abs/2311.12983), [[Leaderboard]](https://huggingface.co/spaces/gaia-benchmark/leaderboard)

- xbench: Tracking Agents Productivity Scaling with Profession-Aligned Real-World Evaluations 
    - [[Paper]](https://arxiv.org/abs/2506.13651)

- MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers 
    - [[Paper]](https://arxiv.org/abs/2508.14704)

- FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction 
    - [[Paper]](https://arxiv.org/abs/2508.11987)

- Terminal-Bench is the benchmark for testing AI agents in real terminal environments 
    - [[GitHub]](https://github.com/laude-institute/terminal-bench)


---
**Last Updated:** Sep 2025  
**Doc Contributor:** Team @ MiroMind AI